# Домашнее задание: Web-scrapping (Нетология)

**Учебный проект для курса "Продвинутый Python" в Нетологии**

## Задание

Парсить страницу со свежими статьями [Хабра](https://habr.com/ru/all/) и выбирать те статьи, в которых встречается хотя бы одно из ключевых слов. Поиск вести по всей доступной preview-информации.

Вывести в консоль список подходящих статей в формате: `<дата> – <заголовок> – <ссылка>`.

Ключевые слова определены в начале скрипта: `['дизайн', 'фото', 'web', 'python']`

## Что сделано

- Реализован скрапер на Python с использованием классов (`requests` + `BeautifulSoup`).
- Источник данных: [Хабр](https://habr.com/ru/all/) — страница со свежими статьями.
- Собираются поля: заголовок, ссылка, дата, превью-текст.
- Поиск по ключевым словам ведется по всей доступной preview-информации.
- Результаты выводятся в консоль в требуемом формате.

## Файлы

- `main.py` — основной исполняемый файл со всей логикой.

## Как запустить

1) Установите зависимости:

```bash
pip install requests beautifulsoup4
```

2) Запустите скрипт:

```bash
python main.py
```

## Коротко о реализации

В `main.py` реализованы продвинутые методы веб-скрапинга, изученные в лекции:

### Основные компоненты:
- `KEYWORDS` — список ключевых слов для поиска.
- Класс `Article` (dataclass) — структура данных статьи с дополнительным полем `found_keywords`.
- Класс `HabrScraper` — продвинутый класс для парсинга с использованием современных методов.

### Используемые методы из лекции:

1. **Регулярные выражения** (`re` модуль):
   - Компилированные паттерны для быстрого поиска ключевых слов
   - Точный поиск с границами слов (`\b`)
   - Игнорирование регистра (`re.IGNORECASE`)

2. **Обработка ошибок и retry логика**:
   - Экспоненциальная задержка между попытками
   - Логирование всех этапов процесса
   - Graceful handling ошибок сети

3. **Продвинутые HTTP-запросы**:
   - Настройка User-Agent и других заголовков
   - Использование Session для переиспользования соединений
   - Проверка Content-Type

4. **Логирование** (`logging` модуль):
   - Запись в файл `scraper.log`
   - Разные уровни логирования (INFO, DEBUG, ERROR)
   - Детальная информация о процессе

5. **Обработка дат** (`datetime` модуль):
   - Парсинг различных форматов дат
   - Fallback на простой формат при ошибках

6. **Продвинутые селекторы BeautifulSoup**:
   - Точные CSS селекторы
   - Поиск в нескольких местах (заголовок, текст, теги, автор)
   - Валидация найденных элементов

### Алгоритм работы:
1) Загружаем страницу с retry логикой и логированием
2) Парсим HTML с помощью BeautifulSoup
3) Для каждой статьи собираем preview-информацию из разных источников
4) Используем регулярные выражения для точного поиска ключевых слов
5) Форматируем даты и выводим результаты
6) Сохраняем результаты в файл с дополнительной информацией

## Дополнительное задание

Для анализа полного текста статей (не только preview) можно расширить класс `HabrScraper` методом, который будет:
1) Переходить по ссылке на статью.
2) Парсить полный текст статьи.
3) Искать ключевые слова в полном тексте.

## Примечания

- Проект оформлен как учебный, в духе решения студента: используются простые классы и явные шаги.
- Поиск ведется без учета регистра.
- Учитывается robots.txt и делаются паузы между запросами.
